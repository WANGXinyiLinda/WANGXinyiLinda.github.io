---
layout: archive
permalink: /
hidden: true
author_profile: true
---

I am Xinyi Wang (王心怡), a final year computer science PhD candidate at University of California, Santa Barbara (UCSB). I am advised by professor [William Yang Wang](https://sites.cs.ucsb.edu/~william/index.html).
I have also worked with [Yi Yang](http://yya518.github.io/), [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/), [Alessandro Sordoni](https://www.microsoft.com/en-us/research/people/alsordon/), [Yikang Shen](https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ), and [Rameswar Pandas](https://rpand002.github.io/). I have interned at [MSR Montreal](https://www.microsoft.com/en-us/research/lab/microsoft-research-montreal/) in 2023 summer, and [MIT-IBM Watson lab](https://mitibmwatsonailab.mit.edu/) in 2024 summer. 
I'm honored to be awarded a J.P. Morgan AI PhD Fellowship.
My research focuses on developing a principled understanding of deep learning models, especially large language models, with the goal of improving their capabilities, addressing their limitations, and optimizing their application across diverse domains.
My CV can be downloaded [here](pdf\CV.pdf). 

<!-- *I'm attending COLM 2024. Please feel free to reach out to me if you want to chat :)* -->

*I'm on the job market right now. My research statement can be found [here](pdf\research_statement.pdf). Please feel free to reach out to me if you think I could be a good fit!*

## Education 
* **University of California, Santa Barbara**, Oct 2020 - Present
  * Ph.D. in Computer Science

* **Hong Kong University of Science and Technology**, Sep 2016 - Jul 2020
  * B.Sc. in Applied Mathematics and Computer Science
  <!-- * CGA: 3.74/4.30  -->
  <!-- \[[transcript](/pdf/HKUST_transcript.pdf)\] -->
  <!-- * Capstone Project Supervisor: Prof. Yuan, Yao  -->

<!-- * **University of California, Los Angeles**, Sep 2019 - Dec 2019
  * Fall quater exchange -->
  <!-- * CGA: 3.90/4.00 (Dean's Honors List)  -->
  <!-- \[[transcript](/pdf/UCLA_transcript.pdf)\] -->


<!-- ## Scholarships and Academic Honors

* Chern Class Talent Scholarship (2017 - 2020) from HKUST Math department
* Chern Class Achievement Scholarship (2020) from HKUST Math department
* The 15th Epsilon Fund Award (2020) from HKUST Math department
* University’s Scholarship Scheme for Continuing Undergraduate Students (2017 - 2020) from HKUST
* Reaching Out Award (2019 - 2020) from HKSAR Government Scholarship Fund
* Joseph Needham Merit Scholarship (2020) from the Joseph Needham Foundation for Science & Civilisation (Hong Kong) 
* Academic Excellence Fellowship (2020) from UCSB -->

> \* indicates equal contribution

## Preprints

* **Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data**

  <u>Xinyi Wang</u>\*, Antonis Antoniades\*, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang Wang

  _Arxiv Preprint_ \[[paper](https://arxiv.org/abs/2407.14985)\]

* **Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement**

  Xunjian Yin, <u>Xinyi Wang</u>, Liangming Pan, Xiaojun Wan, William Yang Wang

  _Arxiv Preprint_ \[[paper](https://arxiv.org/abs/2410.04444)\]

* **Understanding the Interplay between Parametric and Contextual Knowledge for Large Language Models**

  Sitao Cheng, Liangming Pan, Xunjian Yin, <u>Xinyi Wang</u>, William Yang Wang

  _Arxiv Preprint_ \[[paper](https://arxiv.org/abs/2410.08414)\]

## (Co)-First authored publications

* **Guiding Language Model Math Reasoning with Planning Tokens**

  <u>Xinyi Wang</u>, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, Alessandro Sordoni

  _Proceedings of COLM 2024, Philadelphia_ \[[paper](https://arxiv.org/abs/2310.05707)\]\[[code](https://github.com/WANGXinyiLinda/planning_tokens)\]

* **Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation**

  <u>Xinyi Wang</u>, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, William Yang Wang 

  _Proceedings of ICML 2024, Vienna (poster)_ \[[paper](https://arxiv.org/abs/2402.03268)\]\[[code](https://github.com/WANGXinyiLinda/LM_random_walk)\]

* **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning**

  <u>Xinyi Wang</u>, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang

  _Proceedings of NeurIPS 2023, New Orleans (poster)_ \[[paper](http://arxiv.org/abs/2301.11916)\]\[[code](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)\]

* **Causal Balancing for Domain Generalization**

  <u>Xinyi Wang</u>, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang

  _Proceedings of ICLR 2023, Rwanda (poster)_ \[[paper](https://arxiv.org/abs/2206.05263)\]\[[code](https://github.com/WANGXinyiLinda/causal-balancing-for-domain-generalization)\]

* **Counterfactual Maximum Likelihood Estimation for Training Deep Networks** 

  <u>Xinyi Wang</u>, Wenhu Chen, Michael Saxon, William Yang Wang 

  _Proceedings of NeurIPS 2021, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2106.03831)\]\[[code](https://github.com/WANGXinyiLinda/CMLE)\]

* **RefBERT: Compressing BERT by Referencing to Pre-computed Representations** 

  <u>Xinyi Wang</u>\*, Haiqin Yang\*, Liang Zhao, Yang Mo and Jianping Shen 

  _Proceedings of IJCNN 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2106.08898)\]

* **Neural Topic Model with Attention for Supervised Learning** 

  <u>Xinyi Wang</u>, Yi Yang 

   _Proceedings of AISTATS 2020, Virtual (poster)_ \[[paper](http://proceedings.mlr.press/v108/wang20c.html)\]\[[code](https://github.com/WANGXinyiLinda/Neural-Topic-Model-with-Attention-for-Supervised-Learning)\]

## Coauthored publications

* **T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback**

  Jiachen Li, Weixi Feng, Tsu-Jui Fu, <u>Xinyi Wang</u>, Sugato Basu, Wenhu Chen, William Yang Wang

  _Proceedings of NeurIPS 2024, Vancouver_ \[[paper](https://arxiv.org/abs/2405.18750)\]

* **A Survey on Data Selection for Language Models**

  Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, <u>Xinyi Wang</u>, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang

  _TMLR 2024_ \[[paper](https://arxiv.org/abs/2402.16827)\]

* **Position Paper: Understanding the Role of Social Media Influencers in AI Research Visibility**

  Iain Xie Weissburg, Mehir Arora, <u>Xinyi Wang</u>, Liangming Pan, William Yang Wang. 

  _Proceedings of ICML 2024, Vienna_ \[[paper](https://arxiv.org/abs/2401.13782)\]

* **Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies**

  Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, <u>Xinyi Wang</u>, William Yang Wang

  _TACL 2024_ \[[paper](https://arxiv.org/abs/2308.03188)\]

* **Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks**

  Wenhu Chen, Xueguang Ma, <u>Xinyi Wang</u>, William W. Cohen

  _TMLR 2023_ \[[paper](https://arxiv.org/abs/2211.12588)\]\[[code](https://github.com/wenhuchen/Program-of-Thoughts)\]

* **Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**

  Liangming Pan, Alon Albalak, <u>Xinyi Wang</u>, William Yang Wang

  _Findings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.12295)\]\[[code](https://github.com/teacherpeterpan/Logic-LLM)\]

* **TheoremQA: A Theorem-driven Question Answering dataset**

  Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, <u>Xinyi Wang</u>, Pan Lu

  _Proceedings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.12524)\]\[[code](https://github.com/wenhuchen/TheoremQA)\]

* **Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation**

  Wanrong Zhu, <u>Xinyi Wang</u>, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, William Yang Wang

  _Proceedings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.11317)\]

* **PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers**

  Michael Saxon, <u>Xinyi Wang</u>, Wenda Xu, William Yang Wang
  
  _Proceedings of EACL 2023, Croatia_ \[[paper](https://arxiv.org/abs/2112.09237)\]\[[code](https://github.com/michaelsaxon/DatasetAnalysis)\]

* **A Dataset for Answering Time-Sensitive Questions** 

  Wenhu Chen, <u>Xinyi Wang</u>, William Yang Wang 

  _Proceedings of NeurIPS 2021 Datasets and Benchmarks Track, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2108.06314)\]\[[code](https://github.com/wenhuchen/Time-Sensitive-QA)\]

* **Modeling Discolsive Transparency in NLP Application Descriptions** 

  Michael Saxon, Sharon Levy, <u>Xinyi Wang</u>, Alon Albalak, William Yang Wang 

  _Proceedings of EMNLP 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2101.00433)\]\[[code](https://github.com/michaelsaxon/disclosive-transparency)\]

<!-- * **Direct Proof of the Formation of Droplet Surface Shape and the Principle of Minimizing Free Energy** (College Physics. Sep. 2020)

  Kang Jin, **Xinyi Wang**, Kaihang Gui -->

<sub><sup>* indiacts equal contribution</sup></sub>


## Talks
* My PhD major area exam presentation in March 2023: \[[slides](pdf\MAE_online.pdf)\] 
* Talk at Hong Kong University of Science and Technology in May 2023: \[[slides](pdf\llms.pdf)\] 
* Talk at Tsinghua University on October 19, 2023 and at Peking University on October 23, 2023: \[[slides](pdf\oct_talk.pdf)\]
* My PhD proposal presentation in March 2024: \[[slides](pdf\proposal.pdf)\] 


## Services
* Reviewer: NeurIPS Datasets and Benchmarks Track (2021), AAAI (2022, 2023), NeurIPS (2023,2024), ICLR (2024, 2025), ICML (2024), COLM (2024), AISTATS (2025), TPAMI(2024)
* ICLR 2025 Open Science for Foundation Models Workshop organizer