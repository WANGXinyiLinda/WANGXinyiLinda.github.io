---
layout: archive
permalink: /
hidden: true
author_profile: true
---

I am a third-year PhD student in the computer science department at the University of California, Santa Barbara (UCSB). I am advised by professor [William Yang Wang](https://sites.cs.ucsb.edu/~william/index.html).
I am interested in understanding deep learning models, especially pre-trained large language models, using principled causality-based/probabilistic approaches.

*I'm going to intern at MSR Canada at Montreal in Summer 2023. Please feel free to reach out to me if you want to chat about research!*

<!-- *I'm actively looking for a 2023 summer research internship. Please feel free to reach out to me if you think I could be a fit!* -->
<!-- and I have also been working with professor [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/).  -->
<!-- I graduated from the Hong Kong University of Science and Technology (HKUST) in 2020 with a B.Sc. in applied mathematics and computer science.  -->
<!-- I was on exchange at the University of California, Los Angeles (UCLA) from September to December, 2019.  -->
<!-- I also had the fortune to work with [Yi Yang](http://yya518.github.io/) and [Prof. Yuan Yao](https://yao-lab.github.io/).  -->
<!-- \[[CV](/pdf/Resume.pdf)\]  -->

## Education 
* **University of California, Santa Barbara**, Oct 2020 - Present
  * Ph.D. in Computer Science

* **Hong Kong University of Science and Technology**, Sep 2016 - Jul 2020
  * B.Sc. in Applied Mathematics and Computer Science
  <!-- * CGA: 3.74/4.30  -->
  <!-- \[[transcript](/pdf/HKUST_transcript.pdf)\] -->
  <!-- * Capstone Project Supervisor: Prof. Yuan, Yao  -->

* **University of California, Los Angeles**, Sep 2019 - Dec 2019
  * Fall quater exchange
  <!-- * CGA: 3.90/4.00 (Dean's Honors List)  -->
  <!-- \[[transcript](/pdf/UCLA_transcript.pdf)\] -->

## Scholarships and Academic Honors

* Chern Class Talent Scholarship (2017 - 2020) from HKUST Math department
* Chern Class Achievement Scholarship (2020) from HKUST Math department
* The 15th Epsilon Fund Award (2020) from HKUST Math department
* Universityâ€™s Scholarship Scheme for Continuing Undergraduate Students (2017 - 2020) from HKUST
* Reaching Out Award (2019 - 2020) from HKSAR Government Scholarship Fund
* Joseph Needham Merit Scholarship (2020) from the Joseph Needham Foundation for Science & Civilisation (Hong Kong) 
* Academic Excellence Fellowship (2020) from UCSB

## Preprints

* **Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**

  <u>Xinyi Wang</u>, Wanrong Zhu, William Wang

  _Preprint_ \[[paper](http://arxiv.org/abs/2301.11916)\]

* **Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks**

  Wenhu Chen, Xueguang Ma, <u>Xinyi Wang</u>, William W. Cohen

  _Preprint_ \[[paper](https://arxiv.org/abs/2211.12588)\]

## Publications

* **Causal Balancing for Domain Generalization**

  <u>Xinyi Wang</u>, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang

  _Proceedings of ICLR 2023, Rwanda (poster)_ \[[paper](https://arxiv.org/abs/2206.05263)\]

* **PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers**

  Michael Saxon, <u>Xinyi Wang</u>, Wenda Xu, William Yang Wang
  
  _Proceedings of EACL 2023, Croatia_ \[[paper](https://arxiv.org/abs/2112.09237)\]

* **A Dataset for Answering Time-Sensitive Questions** 

  Wenhu Chen, <u>Xinyi Wang</u>, William Yang Wang 

  _Proceedings of NeurIPS 2021 Datasets and Benchmarks Track, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2108.06314)\]\[[code](https://github.com/wenhuchen/Time-Sensitive-QA)\]

* **Counterfactual Maximum Likelihood Estimation for Training Deep Networks** 

  <u>Xinyi Wang</u>, Wenhu Chen, Michael Saxon, William Yang Wang 

  _Proceedings of NeurIPS 2021, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2106.03831)\]\[[code](https://github.com/WANGXinyiLinda/CMLE)\]

* **Modeling Discolsive Transparency in NLP Application Descriptions** 

  Michael Saxon, Sharon Levy, <u>Xinyi Wang</u>, Alon Albalak, William Yang Wang 

  _Proceedings of EMNLP 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2101.00433)\]\[[code](https://github.com/michaelsaxon/disclosive-transparency)\]

* **RefBERT: Compressing BERT by Referencing to Pre-computed Representations** 

  <u>Xinyi Wang</u>\*, Haiqin Yang\*, Liang Zhao, Yang Mo and Jianping Shen 

  _Proceedings of IJCNN 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2106.08898)\]

* **Neural Topic Model with Attention for Supervised Learning** 

  <u>Xinyi Wang</u>, Yi Yang 

   _Proceedings of AISTATS 2020, Virtual (poster)_ \[[paper](http://proceedings.mlr.press/v108/wang20c.html)\]\[[code](https://github.com/WANGXinyiLinda/Neural-Topic-Model-with-Attention-for-Supervised-Learning)\]

<!-- * **Direct Proof of the Formation of Droplet Surface Shape and the Principle of Minimizing Free Energy** (College Physics. Sep. 2020)

  Kang Jin, **Xinyi Wang**, Kaihang Gui -->

<sub><sup>* indiacts equal contribution</sup></sub>

## Talks
* My Major area exam presentation in March 2023: **Understanding Pre-trained Large Language Models through a Probabilistic Lens** \[[slides](pdf\MAE_online.pdf)\]


## Services
* 2021 Program Committee: NeurIPS Datasets and Benchmarks Track
* 2022 Program Committee: AAAI