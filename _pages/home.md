---
layout: archive
permalink: /
hidden: true
author_profile: true
---

I am Xinyi Wang (王心怡), a forth-year computer science PhD student at University of California, Santa Barbara (UCSB). I am advised by professor [William Yang Wang](https://sites.cs.ucsb.edu/~william/index.html).

Before my PhD, I have worked on neural topic models with professor [Yi Yang](http://yya518.github.io/).
I have recently worked with professor [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/) on causality, and [Alessandro Sordoni](https://www.microsoft.com/en-us/research/people/alsordon/) on reasoning as a research intern at MSR.

My current research interest lies in understanding deep learning models, especially language models and other large pre-trained models.
My CV can be downloaded [here](pdf\CV.pdf).

*I'm actively looking for a 2024 summer research internship. Please feel free to reach out to me if you think I could be a good fit!*
<!-- and I have also been working with professor [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/).  -->
<!-- I graduated from the Hong Kong University of Science and Technology (HKUST) in 2020 with a B.Sc. in applied mathematics and computer science.  -->
<!-- I was on exchange at the University of California, Los Angeles (UCLA) from September to December, 2019.  -->
<!-- I also had the fortune to work with [Yi Yang](http://yya518.github.io/) and [Prof. Yuan Yao](https://yao-lab.github.io/).  -->
<!-- \[[CV](/pdf/Resume.pdf)\]  -->

## Education 
* **University of California, Santa Barbara**, Oct 2020 - Present
  * Ph.D. in Computer Science

* **Hong Kong University of Science and Technology**, Sep 2016 - Jul 2020
  * B.Sc. in Applied Mathematics and Computer Science
  <!-- * CGA: 3.74/4.30  -->
  <!-- \[[transcript](/pdf/HKUST_transcript.pdf)\] -->
  <!-- * Capstone Project Supervisor: Prof. Yuan, Yao  -->

* **University of California, Los Angeles**, Sep 2019 - Dec 2019
  * Fall quater exchange
  <!-- * CGA: 3.90/4.00 (Dean's Honors List)  -->
  <!-- \[[transcript](/pdf/UCLA_transcript.pdf)\] -->


## Scholarships and Academic Honors

* Chern Class Talent Scholarship (2017 - 2020) from HKUST Math department
* Chern Class Achievement Scholarship (2020) from HKUST Math department
* The 15th Epsilon Fund Award (2020) from HKUST Math department
* University’s Scholarship Scheme for Continuing Undergraduate Students (2017 - 2020) from HKUST
* Reaching Out Award (2019 - 2020) from HKSAR Government Scholarship Fund
* Joseph Needham Merit Scholarship (2020) from the Joseph Needham Foundation for Science & Civilisation (Hong Kong) 
* Academic Excellence Fellowship (2020) from UCSB

## Preprints

* **Guiding Language Model Reasoning with Planning Tokens**

  <u>Xinyi Wang</u>, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, Alessandro Sordoni

  _Preprint 2022_ \[[paper](https://arxiv.org/abs/2310.05707)\]

* **Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks**

  Wenhu Chen, Xueguang Ma, <u>Xinyi Wang</u>, William W. Cohen

  _Preprint 2022_ \[[paper](https://arxiv.org/abs/2211.12588)\]\[[code](https://github.com/wenhuchen/Program-of-Thoughts)\]

## Publications

* **Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**

  Liangming Pan, Alon Albalak, <u>Xinyi Wang</u>, William Yang Wang

  _Findings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.12295)\]\[[code](https://github.com/teacherpeterpan/Logic-LLM)\]

* **TheoremQA: A Theorem-driven Question Answering dataset**

  Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, <u>Xinyi Wang</u>, Pan Lu

  _Proceedings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.12524)\]\[[code](https://github.com/wenhuchen/TheoremQA)\]

* **Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation**

  Wanrong Zhu, <u>Xinyi Wang</u>, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, William Yang Wang

  _Proceedings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.11317)\]

* **Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning**

  <u>Xinyi Wang</u>, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang

  _Proceedings of NeurIPS 2023, New Orleans (poster)_ \[[paper](http://arxiv.org/abs/2301.11916)\]\[[code](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)\]

* **Causal Balancing for Domain Generalization**

  <u>Xinyi Wang</u>, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang

  _Proceedings of ICLR 2023, Rwanda (poster)_ \[[paper](https://arxiv.org/abs/2206.05263)\]\[[code](https://github.com/WANGXinyiLinda/causal-balancing-for-domain-generalization)\]

* **PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers**

  Michael Saxon, <u>Xinyi Wang</u>, Wenda Xu, William Yang Wang
  
  _Proceedings of EACL 2023, Croatia_ \[[paper](https://arxiv.org/abs/2112.09237)\]\[[code](https://github.com/michaelsaxon/DatasetAnalysis)\]

* **A Dataset for Answering Time-Sensitive Questions** 

  Wenhu Chen, <u>Xinyi Wang</u>, William Yang Wang 

  _Proceedings of NeurIPS 2021 Datasets and Benchmarks Track, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2108.06314)\]\[[code](https://github.com/wenhuchen/Time-Sensitive-QA)\]

* **Counterfactual Maximum Likelihood Estimation for Training Deep Networks** 

  <u>Xinyi Wang</u>, Wenhu Chen, Michael Saxon, William Yang Wang 

  _Proceedings of NeurIPS 2021, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2106.03831)\]\[[code](https://github.com/WANGXinyiLinda/CMLE)\]

* **Modeling Discolsive Transparency in NLP Application Descriptions** 

  Michael Saxon, Sharon Levy, <u>Xinyi Wang</u>, Alon Albalak, William Yang Wang 

  _Proceedings of EMNLP 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2101.00433)\]\[[code](https://github.com/michaelsaxon/disclosive-transparency)\]

* **RefBERT: Compressing BERT by Referencing to Pre-computed Representations** 

  <u>Xinyi Wang</u>\*, Haiqin Yang\*, Liang Zhao, Yang Mo and Jianping Shen 

  _Proceedings of IJCNN 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2106.08898)\]

* **Neural Topic Model with Attention for Supervised Learning** 

  <u>Xinyi Wang</u>, Yi Yang 

   _Proceedings of AISTATS 2020, Virtual (poster)_ \[[paper](http://proceedings.mlr.press/v108/wang20c.html)\]\[[code](https://github.com/WANGXinyiLinda/Neural-Topic-Model-with-Attention-for-Supervised-Learning)\]

<!-- * **Direct Proof of the Formation of Droplet Surface Shape and the Principle of Minimizing Free Energy** (College Physics. Sep. 2020)

  Kang Jin, **Xinyi Wang**, Kaihang Gui -->

<sub><sup>* indiacts equal contribution</sup></sub>

## Talks
* My Major area exam presentation in March 2023: **Understanding Pre-trained Large Language Models through a Probabilistic Lens** \[[slides](pdf\MAE_online.pdf)\] \[[updated slides (May 2023)](pdf\llms.pdf)\] 


## Services
* 2021 Program Committee: NeurIPS Datasets and Benchmarks Track
* 2022 Program Committee: AAAI
* 2023 Program Committee: NeurIPS, AAAI, ICLR