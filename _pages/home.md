---
layout: archive
permalink: /
hidden: true
author_profile: true
---

I am Xinyi Wang (王心怡), an incoming Postdoctoral Researcher at the [Princeton Language and Intelligence Lab](https://pli.princeton.edu/). I recently defended my Ph.D. at the [University of California, Santa Barbara (UCSB)](https://www.cs.ucsb.edu/), where I was advised by [Professor William Yang Wang](https://sites.cs.ucsb.edu/~william/index.html).
In addition, I have had the opportunity to collaborate with [Yi Yang](http://yya518.github.io/), [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/), [Alessandro Sordoni](https://www.microsoft.com/en-us/research/people/alsordon/), [Yikang Shen](https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ), and [Rameswar Pandas](https://rpand002.github.io/).
I am honored to have received the **J.P. Morgan AI Ph.D. Fellowship** and the **UCSB Computer Science Outstanding Publication Award**.
My research centers on developing a principled understanding of large foundation models—particularly large language models (LLMs)—with the aim of enhancing their capabilities, addressing their limitations, and optimizing their deployment across diverse applications.
You can download my CV [here](pdf/CV.pdf).

<span style="color:red">News</span>: I will join the CSE department at [University at Buffalo, SUNY](https://engineering.buffalo.edu/computer-science-engineering.html) in 2026 Fall as an Assistant Professor. I'm recruiting PhD students in the upcoming cycle. 

<!-- *I'm attending ICLR 2025. Please feel free to reach out to me if you want to chat :)* -->

<!-- ## Education 
* **University of California, Santa Barbara**, Oct 2020 - 2025 June
  * Ph.D. in Computer Science

* **Hong Kong University of Science and Technology**, Sep 2016 - Jul 2020
  * B.Sc. in Applied Mathematics and Computer Science -->
  <!-- * CGA: 3.74/4.30  -->
  <!-- \[[transcript](/pdf/HKUST_transcript.pdf)\] -->
  <!-- * Capstone Project Supervisor: Prof. Yuan, Yao  -->

<!-- * **University of California, Los Angeles**, Sep 2019 - Dec 2019
  * Fall quater exchange -->
  <!-- * CGA: 3.90/4.00 (Dean's Honors List)  -->
  <!-- \[[transcript](/pdf/UCLA_transcript.pdf)\] -->


## Preprints
> \* indicates equal contribution

* **Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning**

  <u>Xinyi Wang</u>  , Shawn Tan, Mingyu Jin, William Yang Wang, Rameswar Panda, Yikang Shen

  _Arxiv Preprint_ \[[paper](https://arxiv.org/abs/2504.03635)\]

* **Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement**

  Xunjian Yin, <u>Xinyi Wang</u>, Liangming Pan, Xiaojun Wan, William Yang Wang

  _Arxiv Preprint_ \[[paper](https://arxiv.org/abs/2410.04444)\]

* **Understanding the Interplay between Parametric and Contextual Knowledge for Large Language Models**

  Sitao Cheng, Liangming Pan, Xunjian Yin, <u>Xinyi Wang</u>, William Yang Wang

  _Arxiv Preprint_ \[[paper](https://arxiv.org/abs/2410.08414)\]

## (Co)-First authored publications
> \* indicates equal contribution

* **Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data**

  <u>Xinyi Wang</u>\*, Antonis Antoniades\*, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang Wang

  _Proceedings of ICLR 2025, Singapore (poster)_ \[[paper](https://arxiv.org/abs/2407.14985)\]\[[code](https://github.com/a-antoniades/llm-corpus-search)\]

* **Guiding Language Model Math Reasoning with Planning Tokens**

  <u>Xinyi Wang</u>, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, Alessandro Sordoni

  _Proceedings of COLM 2024, Philadelphia (poster)_ \[[paper](https://arxiv.org/abs/2310.05707)\]\[[code](https://github.com/WANGXinyiLinda/planning_tokens)\]

* **Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation**

  <u>Xinyi Wang</u>, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, William Yang Wang 

  _Proceedings of ICML 2024, Vienna (poster)_ \[[paper](https://arxiv.org/abs/2402.03268)\]\[[code](https://github.com/WANGXinyiLinda/LM_random_walk)\]

* **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning**

  <u>Xinyi Wang</u>, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang

  _Proceedings of NeurIPS 2023, New Orleans (poster)_ \[[paper](http://arxiv.org/abs/2301.11916)\]\[[code](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)\]

* **Causal Balancing for Domain Generalization**

  <u>Xinyi Wang</u>, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang

  _Proceedings of ICLR 2023, Rwanda (poster)_ \[[paper](https://arxiv.org/abs/2206.05263)\]\[[code](https://github.com/WANGXinyiLinda/causal-balancing-for-domain-generalization)\]

* **Counterfactual Maximum Likelihood Estimation for Training Deep Networks** 

  <u>Xinyi Wang</u>, Wenhu Chen, Michael Saxon, William Yang Wang 

  _Proceedings of NeurIPS 2021, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2106.03831)\]\[[code](https://github.com/WANGXinyiLinda/CMLE)\]

* **RefBERT: Compressing BERT by Referencing to Pre-computed Representations** 

  <u>Xinyi Wang</u>\*, Haiqin Yang\*, Liang Zhao, Yang Mo and Jianping Shen 

  _Proceedings of IJCNN 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2106.08898)\]

* **Neural Topic Model with Attention for Supervised Learning** 

  <u>Xinyi Wang</u>, Yi Yang 

   _Proceedings of AISTATS 2020, Virtual (poster)_ \[[paper](http://proceedings.mlr.press/v108/wang20c.html)\]\[[code](https://github.com/WANGXinyiLinda/Neural-Topic-Model-with-Attention-for-Supervised-Learning)\]

## Coauthored publications
> \* indicates equal contribution

* **T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback**

  Jiachen Li, Weixi Feng, Tsu-Jui Fu, <u>Xinyi Wang</u>, Sugato Basu, Wenhu Chen, William Yang Wang

  _Proceedings of NeurIPS 2024, Vancouver (poster)_ \[[paper](https://arxiv.org/abs/2405.18750)\]\[[project](https://t2v-turbo.github.io/)\]

* **A Survey on Data Selection for Language Models**

  Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, <u>Xinyi Wang</u>, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang

  _TMLR 2024_ \[[paper](https://arxiv.org/abs/2402.16827)\]\[[code](https://github.com/alon-albalak/data-selection-survey)\]

* **Position: AI/ML Influencers Have a Place in the Academic Process**

  Iain Xie Weissburg, Mehir Arora, <u>Xinyi Wang</u>, Liangming Pan, William Yang Wang. 

  _Proceedings of ICML 2024, Vienna (poster)_ \[[paper](https://arxiv.org/abs/2401.13782)\]

* **Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies**

  Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, <u>Xinyi Wang</u>, William Yang Wang

  _TACL 2024_ \[[paper](https://arxiv.org/abs/2308.03188)\]\[[code](https://github.com/teacherpeterpan/self-correction-llm-papers)\]

* **Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks**

  Wenhu Chen, Xueguang Ma, <u>Xinyi Wang</u>, William W. Cohen

  _TMLR 2023 (poster)_ \[[paper](https://arxiv.org/abs/2211.12588)\]\[[code](https://github.com/wenhuchen/Program-of-Thoughts)\]

* **Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**

  Liangming Pan, Alon Albalak, <u>Xinyi Wang</u>, William Yang Wang

  _Findings of EMNLP 2023, Singapore (poster)_ \[[paper](https://arxiv.org/abs/2305.12295)\]\[[code](https://github.com/teacherpeterpan/Logic-LLM)\]

* **TheoremQA: A Theorem-driven Question Answering dataset**

  Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, <u>Xinyi Wang</u>, Pan Lu

  _Proceedings of EMNLP 2023, Singapore (poster)_ \[[paper](https://arxiv.org/abs/2305.12524)\]\[[code](https://github.com/wenhuchen/TheoremQA)\]

* **Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation**

  Wanrong Zhu, <u>Xinyi Wang</u>, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, William Yang Wang

  _Proceedings of EMNLP 2023, Singapore (poster)_ \[[paper](https://arxiv.org/abs/2305.11317)\]

* **PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers**

  Michael Saxon, <u>Xinyi Wang</u>, Wenda Xu, William Yang Wang
  
  _Proceedings of EACL 2023, Croatia (poster)_ \[[paper](https://arxiv.org/abs/2112.09237)\]\[[code](https://github.com/michaelsaxon/DatasetAnalysis)\]

* **A Dataset for Answering Time-Sensitive Questions** 

  Wenhu Chen, <u>Xinyi Wang</u>, William Yang Wang 

  _Proceedings of NeurIPS 2021 Datasets and Benchmarks Track, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2108.06314)\]\[[code](https://github.com/wenhuchen/Time-Sensitive-QA)\]

* **Modeling Discolsive Transparency in NLP Application Descriptions** 

  Michael Saxon, Sharon Levy, <u>Xinyi Wang</u>, Alon Albalak, William Yang Wang 

  _Proceedings of EMNLP 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2101.00433)\]\[[code](https://github.com/michaelsaxon/disclosive-transparency)\]

<!-- * **Direct Proof of the Formation of Droplet Surface Shape and the Principle of Minimizing Free Energy** (College Physics. Sep. 2020)

  Kang Jin, **Xinyi Wang**, Kaihang Gui -->

<sub><sup>* indiacts equal contribution</sup></sub>


## Talks
* My PhD major area exam presentation in March 2023: \[[slides](pdf\MAE_online.pdf)\] 
* Talk at Hong Kong University of Science and Technology in May 2023: \[[slides](pdf\llms.pdf)\] 
* Talk at Tsinghua University on October 19, 2023 and at Peking University on October 23, 2023: \[[slides](pdf\oct_talk.pdf)\]
* My PhD proposal presentation in March 2024: \[[slides](pdf\proposal.pdf)\] 


## Services
* Reviewer: NeurIPS, AAAI, NeurIPS, ICLR, ICML, COLM, AISTATS, TPAMI, TMLR
* Organizer: ICLR 2025 Open Science for Foundation Models Workshop 