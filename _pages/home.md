---
layout: archive
permalink: /
hidden: true
author_profile: true
---

I am Xinyi Wang (王心怡), a forth-year computer science PhD candidate at University of California, Santa Barbara (UCSB). I am advised by professor [William Yang Wang](https://sites.cs.ucsb.edu/~william/index.html).
I have also worked with [Yi Yang](http://yya518.github.io/), [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/), and [Alessandro Sordoni](https://www.microsoft.com/en-us/research/people/alsordon/). My current research interest lies in improving and making better use of foundation models by developing a principled understanding of them.
My CV can be downloaded [here](pdf\CV.pdf).

<!-- *I'm actively looking for a 2024 summer research internship. Please feel free to reach out to me if you think I could be a good fit!* -->
<!-- and I have also been working with professor [Kun Zhang](https://www.andrew.cmu.edu/user/kunz1/).  -->
<!-- I graduated from the Hong Kong University of Science and Technology (HKUST) in 2020 with a B.Sc. in applied mathematics and computer science.  -->
<!-- I was on exchange at the University of California, Los Angeles (UCLA) from September to December, 2019.  -->
<!-- I also had the fortune to work with [Yi Yang](http://yya518.github.io/) and [Prof. Yuan Yao](https://yao-lab.github.io/).  -->
<!-- \[[CV](/pdf/Resume.pdf)\]  -->

## Education 
* **University of California, Santa Barbara**, Oct 2020 - Present
  * Ph.D. in Computer Science

* **Hong Kong University of Science and Technology**, Sep 2016 - Jul 2020
  * B.Sc. in Applied Mathematics and Computer Science
  <!-- * CGA: 3.74/4.30  -->
  <!-- \[[transcript](/pdf/HKUST_transcript.pdf)\] -->
  <!-- * Capstone Project Supervisor: Prof. Yuan, Yao  -->

<!-- * **University of California, Los Angeles**, Sep 2019 - Dec 2019
  * Fall quater exchange -->
  <!-- * CGA: 3.90/4.00 (Dean's Honors List)  -->
  <!-- \[[transcript](/pdf/UCLA_transcript.pdf)\] -->


<!-- ## Scholarships and Academic Honors

* Chern Class Talent Scholarship (2017 - 2020) from HKUST Math department
* Chern Class Achievement Scholarship (2020) from HKUST Math department
* The 15th Epsilon Fund Award (2020) from HKUST Math department
* University’s Scholarship Scheme for Continuing Undergraduate Students (2017 - 2020) from HKUST
* Reaching Out Award (2019 - 2020) from HKSAR Government Scholarship Fund
* Joseph Needham Merit Scholarship (2020) from the Joseph Needham Foundation for Science & Civilisation (Hong Kong) 
* Academic Excellence Fellowship (2020) from UCSB -->

## Preprints

* **A Survey on Data Selection for Language Models**

  Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, <u>Xinyi Wang</u>, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang

  _Preprint 2024_ \[[paper](https://arxiv.org/abs/2402.16827)\]

* **Guiding Language Model Reasoning with Planning Tokens**

  <u>Xinyi Wang</u>, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, Alessandro Sordoni

  _Preprint 2023_ \[[paper](https://arxiv.org/abs/2310.05707)\]

## First authored publications

* **Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation**

  <u>Xinyi Wang</u>, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, William Yang Wang 

  _ICML 2024, Vienna_ \[[paper](https://arxiv.org/abs/2402.03268)\]

* **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning**

  <u>Xinyi Wang</u>, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang

  _Proceedings of NeurIPS 2023, New Orleans (poster)_ \[[paper](http://arxiv.org/abs/2301.11916)\]\[[code](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)\]

* **Causal Balancing for Domain Generalization**

  <u>Xinyi Wang</u>, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang

  _Proceedings of ICLR 2023, Rwanda (poster)_ \[[paper](https://arxiv.org/abs/2206.05263)\]\[[code](https://github.com/WANGXinyiLinda/causal-balancing-for-domain-generalization)\]

* **Counterfactual Maximum Likelihood Estimation for Training Deep Networks** 

  <u>Xinyi Wang</u>, Wenhu Chen, Michael Saxon, William Yang Wang 

  _Proceedings of NeurIPS 2021, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2106.03831)\]\[[code](https://github.com/WANGXinyiLinda/CMLE)\]

* **RefBERT: Compressing BERT by Referencing to Pre-computed Representations** 

  <u>Xinyi Wang</u>\*, Haiqin Yang\*, Liang Zhao, Yang Mo and Jianping Shen 

  _Proceedings of IJCNN 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2106.08898)\]

* **Neural Topic Model with Attention for Supervised Learning** 

  <u>Xinyi Wang</u>, Yi Yang 

   _Proceedings of AISTATS 2020, Virtual (poster)_ \[[paper](http://proceedings.mlr.press/v108/wang20c.html)\]\[[code](https://github.com/WANGXinyiLinda/Neural-Topic-Model-with-Attention-for-Supervised-Learning)\]

## Coauthored publications

* **Position Paper: Understanding the Role of Social Media Influencers in AI Research Visibility**

  Iain Xie Weissburg, Mehir Arora, <u>Xinyi Wang</u>, Liangming Pan, William Yang Wang. 

  _ICML 2024, Vienna_ \[[paper](https://arxiv.org/abs/2401.13782)\]

* **Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies**

  Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, <u>Xinyi Wang</u>, William Yang Wang

  _TACL 2024_ \[[paper](https://arxiv.org/abs/2308.03188)\]

* **Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks**

  Wenhu Chen, Xueguang Ma, <u>Xinyi Wang</u>, William W. Cohen

  _TMLR 2023_ \[[paper](https://arxiv.org/abs/2211.12588)\]\[[code](https://github.com/wenhuchen/Program-of-Thoughts)\]

* **Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning**

  Liangming Pan, Alon Albalak, <u>Xinyi Wang</u>, William Yang Wang

  _Findings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.12295)\]\[[code](https://github.com/teacherpeterpan/Logic-LLM)\]

* **TheoremQA: A Theorem-driven Question Answering dataset**

  Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, <u>Xinyi Wang</u>, Pan Lu

  _Proceedings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.12524)\]\[[code](https://github.com/wenhuchen/TheoremQA)\]

* **Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation**

  Wanrong Zhu, <u>Xinyi Wang</u>, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, William Yang Wang

  _Proceedings of EMNLP 2023, Singapore_ \[[paper](https://arxiv.org/abs/2305.11317)\]

* **PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers**

  Michael Saxon, <u>Xinyi Wang</u>, Wenda Xu, William Yang Wang
  
  _Proceedings of EACL 2023, Croatia_ \[[paper](https://arxiv.org/abs/2112.09237)\]\[[code](https://github.com/michaelsaxon/DatasetAnalysis)\]

* **A Dataset for Answering Time-Sensitive Questions** 

  Wenhu Chen, <u>Xinyi Wang</u>, William Yang Wang 

  _Proceedings of NeurIPS 2021 Datasets and Benchmarks Track, Virtual (poster)_ \[[paper](https://arxiv.org/abs/2108.06314)\]\[[code](https://github.com/wenhuchen/Time-Sensitive-QA)\]

* **Modeling Discolsive Transparency in NLP Application Descriptions** 

  Michael Saxon, Sharon Levy, <u>Xinyi Wang</u>, Alon Albalak, William Yang Wang 

  _Proceedings of EMNLP 2021, Virtual (<span style="color:red">oral</span>)_ \[[paper](https://arxiv.org/abs/2101.00433)\]\[[code](https://github.com/michaelsaxon/disclosive-transparency)\]

<!-- * **Direct Proof of the Formation of Droplet Surface Shape and the Principle of Minimizing Free Energy** (College Physics. Sep. 2020)

  Kang Jin, **Xinyi Wang**, Kaihang Gui -->

<sub><sup>* indiacts equal contribution</sup></sub>


## Talks
* My PhD major area exam presentation in March 2023: \[[slides](pdf\MAE_online.pdf)\] 
* Talk at Hong Kong University of Science and Technology in May 2023: \[[slides](pdf\llms.pdf)\] 
* Talk at Tsinghua University on October 19, 2023 and at Peking University on October 23, 2023: \[[slides](pdf\oct_talk.pdf)\]
* My PhD proposal presentation in March 2024: \[[slides](pdf\proposal.pdf)\] 


## Services
* Reviewer: NeurIPS Datasets and Benchmarks Track (2021), AAAI (2022, 2023), NeurIPS (2023), ICLR (2024), ICML (2024), COLM (2024), TPAMI(2024)